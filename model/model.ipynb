{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_3d_unet(input_shape=(128,128,32,4)):\n",
    "  inputs= keras.layers.Input(shape=input_shape)\n",
    "\n",
    "  ## convolutional layers\n",
    "  conv_layer1 = keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(inputs)\n",
    "  conv_layer2 = keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(conv_layer1)\n",
    "\n",
    "  ## add max pooling to obtain the most imformatic features\n",
    "  pooling_layer1 = keras.layers.MaxPool3D(pool_size=(2, 2, 2),padding=\"same\")(conv_layer2)\n",
    "\n",
    "  conv_layer3 = keras.layers.Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(pooling_layer1)\n",
    "  pooling_layer2 = keras.layers.MaxPool3D(pool_size=(2, 2, 2),padding=\"same\")(conv_layer3)\n",
    "  conv_layer4 = keras.layers.Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(pooling_layer2)\n",
    "  upsamp_layer1=keras.layers.UpSampling3D(size=(2, 2, 2))(conv_layer4)\n",
    "  conv_layer5 = keras.layers.Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(upsamp_layer1)\n",
    "  upsamp_layer2=keras.layers.UpSampling3D(size=(2, 2, 2))(conv_layer5)\n",
    "  conv_layer6 = keras.layers.Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(upsamp_layer2)\n",
    "  conv_layer7 = keras.layers.Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu',padding=\"same\")(conv_layer6)\n",
    "  #cropped_output = keras.layers.Cropping3D(cropping=((0, 0), (0, 0), (0, 1)))(conv_layer7)  # Remove 1 voxel from depth\n",
    "  ## now we perform up sampling\n",
    "  outputs= keras.layers.Conv3D(filters=1, kernel_size=(1,1,1), activation='sigmoid')(conv_layer7)\n",
    "  model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(h5_file_path, batch_size):\n",
    "    with h5py.File(h5_file_path, 'r') as hf:\n",
    "        X = hf['X']\n",
    "        Y = hf['Y']\n",
    "        num_samples = X.shape[0]\n",
    "        train_samples=int(num_samples*0.8)\n",
    "\n",
    "        while True:  \n",
    "            for i in range(0, train_samples, batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                Y_batch = Y[i:i + batch_size]\n",
    "                yield X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=4, \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'model-unet.best.keras',  \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',  \n",
    "    factor=0.1,         \n",
    "    patience=3,\n",
    "    min_lr=1e-6        \n",
    ")\n",
    "def loss_history_callback(logs=None):\n",
    "    print(f\"Epoch {8 + 1}:\")\n",
    "    print(f\"  Binary Crossentropy Loss: {logs['output_layer_name_loss']}\")\n",
    "    print(f\"  Unified Focal Loss: {logs['custom_loss_loss']}\")\n",
    "    print(f\"  Combined Loss: {logs['loss']}\")\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    model_checkpoint,\n",
    "    tensorboard_callback,\n",
    "    reduce_lr\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    # Flatten the tensors\n",
    "    y_true=tf.cast(y_true,tf.float32)\n",
    "    y_pred=tf.cast(y_pred,tf.float32)\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    \n",
    "    # Calculate intersection\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    \n",
    "    # Calculate Dice coefficient\n",
    "    dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    # Flatten the tensors\n",
    "    y_true=tf.cast(y_true,tf.float32)\n",
    "    y_pred=tf.cast(y_pred,tf.float32)\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "    return 1 - dice\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
